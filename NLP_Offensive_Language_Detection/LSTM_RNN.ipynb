{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 5.2MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.31.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding over ...\n",
      "---------------Prepare data for task a---------------\n",
      "---------You are requiring train data!---------\n",
      "torch.Size([13240, 105])\n",
      "lstm embed_size: 10  hiddendim: 6\n",
      "Epoch:  01 | Train accuracy:  66.73% | Valid acc:  66.83%\n",
      "Epoch:  02 | Train accuracy:  87.62% | Valid acc:  72.37%\n",
      "Epoch:  03 | Train accuracy:  92.87% | Valid acc:  73.07%\n",
      "Epoch:  04 | Train accuracy:  94.78% | Valid acc:  74.37%\n",
      "Epoch:  05 | Train accuracy:  96.30% | Valid acc:  73.40%\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "from data_process import DataHandle, get_task_data\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import random\n",
    "print('Libraries imported!')\n",
    "\n",
    "# we fix the seeds to get consistent results\n",
    "SEED = 234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "def word2vec_embedding(obj, embed_size=50, min_count=1, window=5):\n",
    "    sentences = obj.tokenized_corpus\n",
    "    model = Word2Vec(sentences,min_count=min_count, window=window, size=embed_size)\n",
    "    # model.build_vocab(sentences)  # prepare the model vocabulary\n",
    "    # train word vectors\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    # add the first vector as pading\n",
    "    embed_vectors = np.vstack([np.zeros((1, embed_size)), model.wv.vectors])\n",
    "    return embed_vectors\n",
    "def get_model_inputs(tokenized_corpus, word2idx, labels, max_len):\n",
    "    # we index our sentences\n",
    "    vectorized_sents = [[word2idx[tok] for tok in sent if tok in word2idx] for sent in tokenized_corpus]\n",
    "    # we create a tensor of a fixed size filled with zeroes for padding\n",
    "    sent_tensor = Variable(torch.zeros((len(vectorized_sents), max_len))).long()\n",
    "    sent_lengths = [len(sent) for sent in vectorized_sents]\n",
    "    # we fill it with our vectorized sentences\n",
    "    for idx, (sent, sentlen) in enumerate(zip(vectorized_sents, sent_lengths)):\n",
    "        sent_tensor[idx, :sentlen] = torch.LongTensor(sent)\n",
    "    label_tensor = torch.FloatTensor(labels)\n",
    "    return sent_tensor, label_tensor\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_len, num_classes):\n",
    "        super(LSTM,self).__init__()\n",
    "        # embedding (lookup layer) layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # hidden layer\n",
    "#         self.lstm = nn.LSTM(embedding_dim,hidden_dim,1,bidirectional=True,dropout = 0.1)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,1,bidirectional=True)\n",
    "        # output layer\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.out = nn.Linear(hidden_dim*2*2, num_classes)\n",
    "#         self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "#         print(embedded)\n",
    "        states, hidden = self.lstm(embedded.permute([1, 0, 2]))\n",
    "        encoding = torch.cat([states[0],states[-1]], dim=1)\n",
    "        out = self.out(encoding)\n",
    "#         print(encoding.shape)\n",
    "\n",
    "#         states=states.permute([1, 0, 2])\n",
    "#         states=states.reshape(states.shape[0],-1)\n",
    "#         print(states[20])\n",
    "#         out = self.out(states[30])\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def accuracy(output, target):\n",
    "    predict = torch.round(torch.sigmoid(output))\n",
    "#     print(predict)\n",
    "    correct = (predict == target).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def train_lstm(embeddingw,embed_size,train_sent_tensor, train_label_tensor, valid_sent_tensor, valid_label_tensor,\n",
    "               epochs=10, Vocabulary=0, EMBEDDING_DIM=15, HIDDEN_DIM=8, OUTPUT_DIM=1, max_len=0, lr=0.01, batch=64):\n",
    "\n",
    "    model = LSTM(EMBEDDING_DIM, HIDDEN_DIM, Vocabulary, max_len, OUTPUT_DIM)\n",
    "    model.embedding.weight.data.copy_(torch.from_numpy(embeddingw)) #use own embedding\n",
    "    model.embedding.weight.require_grad = False\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    num = len(train_label_tensor) // batch\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # to ensure the dropout (exlained later) is \"turned on\" while training\n",
    "        # good practice to include even if do not use here\n",
    "        np.random.seed(SEED)\n",
    "        p = np.random.permutation(len(train_label_tensor))\n",
    "        train_sent_tensor, train_label_tensor = train_sent_tensor[p], train_label_tensor[p]\n",
    "        epoch_loss = 0\n",
    "        for i in range(num):\n",
    "            feature = train_sent_tensor[i * batch:(i+1) * batch]\n",
    "            target = train_label_tensor[i * batch:(i+1) * batch]\n",
    "#             print(feature)\n",
    "            model.train()\n",
    "            # we zero the gradients as they are not removed automatically\n",
    "            optimizer.zero_grad()\n",
    "            # queeze is needed as the predictions are initially size (batch size, 1) and we need to remove the dimension of size 1\n",
    "            predictions = model(feature).squeeze(1)\n",
    "#             print(predictions)\n",
    "#             print(predictions)\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # calculate the gradient of each parameter\n",
    "            loss.backward()\n",
    "            # update the parameters using the gradients and optimizer algorithm\n",
    "            optimizer.step()\n",
    "            batch_loss = loss.item()\n",
    "            # print(f'| Epoch: {epoch:02} | Batch: {i: 04} | Train Loss: {batch_loss:.3f}')\n",
    "        predict = model(train_sent_tensor).squeeze(1)\n",
    "        predict_val = model(valid_sent_tensor).squeeze(1)\n",
    "        train_acc = accuracy(predict, train_label_tensor)\n",
    "        valid_acc = accuracy(predict_val, valid_label_tensor)\n",
    "#         print(predict_val.shape)\n",
    "        print(f'Epoch: {epoch: 03} | Train accuracy: {train_acc * 100: .2f}% | Valid acc: {valid_acc * 100: .2f}%')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import pickle\n",
    "    with open('data_object.pkl', 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "        f.close()\n",
    "#     obj = DataHandle()\n",
    "    embed_size = 10\n",
    "    hiddendim=6\n",
    "    lr=0.02\n",
    "    epochs=5\n",
    "    embedding = word2vec_embedding(obj, embed_size=embed_size)\n",
    "    print('embedding over ...')\n",
    "    tokenized_corpus = obj.tokenized_corpus\n",
    "    train, train_labels = get_task_data(obj, train=True, task='a')\n",
    "\n",
    "    sent_lengths = [len(sent) for sent in tokenized_corpus]\n",
    "    max_len = np.max(np.array(sent_lengths))\n",
    "\n",
    "    word2idx = obj.word2idx\n",
    "\n",
    "    train_sent_tensor, train_label_tensor = get_model_inputs(tokenized_corpus, word2idx, train_labels, max_len)\n",
    "    print(train_sent_tensor.shape)\n",
    "    print('lstm embed_size:',embed_size,' hiddendim:',hiddendim)\n",
    "    train_lstm(embedding,embed_size,train_sent_tensor[:10000], train_label_tensor[:10000], train_sent_tensor[-3000:], train_label_tensor[-3000:],\n",
    "               epochs=epochs, lr=lr, Vocabulary=len(word2idx),EMBEDDING_DIM=embed_size,HIDDEN_DIM=hiddendim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2, 3]), array([2, 3]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=np.array([1,2])\n",
    "y=x\n",
    "y+=1\n",
    "print(y,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
